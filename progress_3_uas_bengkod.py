# -*- coding: utf-8 -*-
"""Progress 3 UAS Bengkod

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10IMY8hq-1k8RQYRMkBMPCRi5BRGoFiB-
"""

# 1. Exploratory Data Analysis (EDA)
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load data
df = pd.read_csv("ObesityDataSet.csv")

# Tampilkan beberapa baris pertama dan informasi umum
print("\nHead of dataset:")
display(df.head())
print("\nDataset info:")
print(df.info())
print("\nDescriptive statistics:")
display(df.describe(include='all'))

# Visualisasi distribusi target
plt.figure(figsize=(10, 5))
sns.countplot(data=df, x='NObeyesdad', order=df['NObeyesdad'].value_counts().index)
plt.xticks(rotation=45)
plt.title("Distribusi Kelas Target")
plt.tight_layout()
plt.show()

# Cek missing values, unique values, duplikasi, keseimbangan data
print("\nMissing Values:\n", df.isnull().sum())
print("\nUnique Values:\n", df.nunique())
print("\nDuplicated Rows:", df.duplicated().sum())

# Boxplot untuk deteksi outlier
df_numeric = df.select_dtypes(include='number')
plt.figure(figsize=(15, 10))
for i, col in enumerate(df_numeric.columns):
    plt.subplot(3, 3, i+1)
    sns.boxplot(x=df[col])
    plt.title(f"Boxplot of {col}")
plt.tight_layout()
plt.show()
# Kesimpulan EDA: Data terdiri dari fitur numerik dan kategorikal,
# terdapat duplikasi, potensi outlier, serta imbalance di target class.

# 2. Preprocessing Data
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder, StandardScaler
from imblearn.over_sampling import SMOTE

# Salin data
df_clean = df.copy()

# Hapus duplikat
df_clean.drop_duplicates(inplace=True)

# Pisahkan kolom numerik dan kategorikal
num_cols = df_clean.columns[df_clean.apply(pd.to_numeric, errors='coerce').notnull().all()].tolist()

cat_cols = df_clean.select_dtypes(include='object').drop('NObeyesdad', axis=1).columns

# Imputasi missing values
num_imputer = SimpleImputer(strategy='mean')
df_clean[num_cols] = df_clean[num_cols].apply(pd.to_numeric, errors='coerce')


cat_imputer = SimpleImputer(strategy='most_frequent')
df_clean[cat_cols] = cat_imputer.fit_transform(df_clean[cat_cols])

# Tangani outlier dengan IQR
for col in num_cols:
    Q1 = df_clean[col].quantile(0.25)
    Q3 = df_clean[col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    df_clean = df_clean[(df_clean[col] >= lower) & (df_clean[col] <= upper)]

# Encode fitur kategorikal
encoder = LabelEncoder()
for col in cat_cols:
    df_clean[col] = encoder.fit_transform(df_clean[col])
df_clean['NObeyesdad'] = encoder.fit_transform(df_clean['NObeyesdad'])

# Gunakan semua fitur
X = df_clean.drop('NObeyesdad', axis=1)
y = df_clean['NObeyesdad']

# Tangani imbalance dengan SMOTE
sm = SMOTE(random_state=42)
X_resampled, y_resampled = sm.fit_resample(X, y)

# Normalisasi fitur
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_resampled)

# Kesimpulan preprocessing: duplikasi, missing, dan outlier telah ditangani,
# data telah diencoding, diseimbangkan dan dinormalisasi.

# 3. Pemodelan dan Evaluasi
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_resampled, test_size=0.2, random_state=42)

models = {
    'Random Forest': RandomForestClassifier(),
    'KNN': KNeighborsClassifier(),
    'Logistic Regression': LogisticRegression(max_iter=500)
}

results = {}

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    results[name] = {
        'Accuracy': accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred, average='weighted'),
        'Recall': recall_score(y_test, y_pred, average='weighted'),
        'F1-Score': f1_score(y_test, y_pred, average='weighted'),
        'Confusion Matrix': confusion_matrix(y_test, y_pred)
    }

for name, metrics in results.items():
    print(f"\n{name} Results:")
    print(f"Accuracy: {metrics['Accuracy']:.4f}")
    print(f"Precision: {metrics['Precision']:.4f}")
    print(f"Recall: {metrics['Recall']:.4f}")
    print(f"F1-Score: {metrics['F1-Score']:.4f}")
    print("Confusion Matrix:\n", metrics['Confusion Matrix'])

# Visualisasi performa
scores_df = pd.DataFrame(results).T[['Accuracy', 'Precision', 'Recall', 'F1-Score']]
scores_df.plot(kind='bar', figsize=(10, 6), ylim=(0, 1), title='Model Performance Comparison')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Kesimpulan pemodelan: Random Forest memberikan hasil terbaik dari semua metrik.

# 4. Hyperparameter Tuning
from sklearn.model_selection import GridSearchCV

param_grid_rf = {
    'n_estimators': [50, 100, 150],
    'max_depth': [None, 10, 20]
}
grid = GridSearchCV(RandomForestClassifier(), param_grid_rf, cv=3, scoring='accuracy')
grid.fit(X_train, y_train)
best_rf = grid.best_estimator_

# Evaluasi ulang
y_pred_best = best_rf.predict(X_test)
print("\nTuned Random Forest Results:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_best):.4f}")
print(f"F1-Score: {f1_score(y_test, y_pred_best, average='weighted'):.4f}")

# Visualisasi peningkatan performa
before_tuning = results['Random Forest']
after_tuning = {
    'Accuracy': accuracy_score(y_test, y_pred_best),
    'F1-Score': f1_score(y_test, y_pred_best, average='weighted')
}

improvement_df = pd.DataFrame([before_tuning, after_tuning], index=['Before Tuning', 'After Tuning'])[['Accuracy', 'F1-Score']]
improvement_df.plot(kind='bar', figsize=(8, 5), ylim=(0, 1), title='Random Forest Performance Before & After Tuning')
plt.xticks(rotation=0)
plt.tight_layout()
plt.show()

# Kesimpulan tuning: GridSearchCV berhasil meningkatkan performa model Random Forest.

# 5. Deployment (Streamlit)
import joblib
joblib.dump(best_rf, 'best_model.pkl')
joblib.dump(scaler, 'scaler.pkl')

with open("app.py", "w") as f:
    f.write('''
import streamlit as st
import joblib
import numpy as np

model = joblib.load('best_model.pkl')
scaler = joblib.load('scaler.pkl')

st.title("Prediksi Obesitas")

cols = ["Age", "Height", "Weight", "FCVC", "NCP", "CH2O", "FAF", "TUE", "Gender", "family_history_with_overweight"]
inputs = []
for col in cols:
    value = st.number_input(f"Masukkan nilai untuk {col}", step=0.1)
    inputs.append(value)

if st.button("Prediksi"):
    inputs_scaled = scaler.transform([inputs])
    prediction = model.predict(inputs_scaled)
    st.write(f"Prediksi: {prediction[0]}")
''')

# 6. Kesimpulan
# Pipeline klasifikasi obesitas dibangun lengkap dari EDA, preprocessing,
# modeling, tuning, hingga deployment. Random Forest dengan hyperparameter
# tuning memberikan performa terbaik. Aplikasi siap dideploy secara online.